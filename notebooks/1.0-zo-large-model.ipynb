{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46205060-0262-424a-9f4f-fe7fa6a8da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from itertools import count\n",
    "import natsort\n",
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3db84d60-f552-4339-bd00-dc311866e3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1064713b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import glob\n",
    "import numpy\n",
    "import random\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "torch.manual_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d8c95d6-06df-4f9d-8149-a320381267c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc2e6863-1b3f-4fad-a3ee-5e4a4e03b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=False):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels    #.astype(dtype='int')\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filepath = self.image_paths[idx]\n",
    "        image = cv2.imread(image_filepath)\n",
    "        label = self.labels[idx]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e99222-8446-455f-b684-e11a006eacab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(model_name):\n",
    "    transform = A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ToTensorV2(),\n",
    "        ])  \n",
    "    \n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b8890e7-46d7-4e7d-924d-1aa6782a9444",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_paths = []\n",
    "filename = '/Users/zihuiouyang/Downloads/images/training/notflip'\n",
    "train_image_paths.append(glob.glob(filename + '/*'))\n",
    "filename1 = '/Users/zihuiouyang/Downloads/images/training/flip'\n",
    "train_image_paths.append(glob.glob(filename1 + '/*'))\n",
    "train_image_paths1 = [item for sublist in train_image_paths for item in sublist]\n",
    "train_image_paths1 = natsort.natsorted(train_image_paths1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d602089b-6ca4-4862-af25-c675bb652f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(1230):\n",
    "    l.append(0)\n",
    "for i in range(1162):\n",
    "    l.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "499c8d8f-569f-4ff7-9061-4e4e0c1fef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_list = pd.DataFrame(\n",
    "    {'Link': train_image_paths1,\n",
    "     'Label': l,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3ed5b29-7c07-4e4c-ab36-a889f6c9cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_list1 = percentile_list.sample(frac=1, random_state=1)\n",
    "train_image_paths = percentile_list1.loc[:,\"Link\"].to_numpy().tolist()\n",
    "labels = percentile_list1.loc[:,\"Label\"].to_numpy().tolist()\n",
    "train_image_paths, valid_image_paths = train_image_paths[:int(0.8*len(train_image_paths))], train_image_paths[int(0.8*len(train_image_paths)):]  \n",
    "train_labels, valid_labels = labels[:int(0.8*len(labels))], labels[int(0.8*len(labels)):] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd75a94a-bbbc-4919-8d56-6b1c66942ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {i:0 for i in range(2)}\n",
    "num_classes = 2\n",
    "total_samples = 0\n",
    "for i in train_labels:\n",
    "    total_samples += 1\n",
    "    summary[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "245c2aba-d2c9-4f1c-8c8c-8d553e00c077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier():\n",
    "\n",
    "    def __init__(self, name, model, dataloaders, parameter, use_cuda=False):\n",
    "        \n",
    "        '''\n",
    "        @name: Experiment name. Will define stored results etc. \n",
    "        @model: Any models\n",
    "        @dataloaders: Dictionary with keys train, val and test and corresponding dataloaders\n",
    "        @class_names: list of classes, where the idx of class name corresponds to the label used for it in the data\n",
    "        @use_cuda: whether or not to use cuda\n",
    "        '''\n",
    "       \n",
    "        self.name = name\n",
    "        if use_cuda and not torch.backends.mps.is_available():\n",
    "            raise Exception(\"Asked for MPS but not found\")\n",
    "            \n",
    "        self.use_cuda = use_cuda\n",
    "        self.epoch = parameter['epochs']\n",
    "        self.lr = parameter['lr']\n",
    "        self.batch_size = parameter['batch_size']\n",
    "        \n",
    "        self.model = model.to('mps' if use_cuda else 'cpu') # model.to('cpu')\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.train_loader, self.valid_loader = self.get_dataloaders(dataloaders['train_image_paths'], \n",
    "                                                                    dataloaders['train_labels'], \n",
    "                                                                    dataloaders['valid_image_paths'], \n",
    "                                                                    dataloaders['valid_labels'], \n",
    "                                                                    train_transforms=dataloaders['transforms'], \n",
    "                                                                    batch_size = self.batch_size,\n",
    "                                                                    shuffle=parameter['shuffle'],\n",
    "                                                                   )\n",
    "        self.class_names = parameter['class_names']\n",
    "        \n",
    "        self.activations_path = os.path.join('activations', self.name)\n",
    "        self.kernel_path = os.path.join('kernel_viz', self.name)\n",
    "        save_path = \"/Users/zihuiouyang/Downloads/\"\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        if not os.path.exists(self.activations_path):\n",
    "            os.makedirs(self.activations_path)\n",
    "\n",
    "        if not os.path.exists(self.kernel_path):\n",
    "            os.makedirs(self.kernel_path)\n",
    "            \n",
    "        self.save_path = save_path\n",
    "\n",
    "    def train(self, save=True):\n",
    "        '''\n",
    "        @epochs: number of epochs to train\n",
    "        @save: whether or not to save the checkpoints\n",
    "        '''\n",
    "        best_val_accuracy = - math.inf\n",
    "        \n",
    "        for epoch in range(self.epoch):  # loop over the dataset multiple times\n",
    "            self.model.train()\n",
    "            t = time.time()\n",
    "            running_loss = 0.0\n",
    "            train_acc = 0\n",
    "            val_accuracy = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            count = 0\n",
    "            loop = tqdm.tqdm(self.train_loader, total = len(self.train_loader), leave = True)\n",
    "            \n",
    "            for img, label in loop:\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs, labels = img.to(device), label.to(device) #img.to(device), label.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = self.model(inputs)\n",
    "                _, predictions = torch.max(outputs, 1)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                total += labels.shape[0]\n",
    "                correct += (predictions == labels).sum().item()\n",
    "                \n",
    "                count += 1\n",
    "                if count % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                    print(f'[{epoch + 1}, {count + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "                    running_loss = 0.0\n",
    "                    \n",
    "            train_acc = 100 * correct / total\n",
    "            print(f'Epoch:', epoch + 1, f'Training Epoch Accuracy:{train_acc}')\n",
    "            \n",
    "            # evaluate the validation dataset\n",
    "            self.model.eval()\n",
    "            correct_pred = {classname: 0 for classname in self.class_names}\n",
    "            total_pred = {classname: 0 for classname in self.class_names}\n",
    "\n",
    "            # again no gradients needed\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for data in self.valid_loader:\n",
    "                    images, labels = data[0].to(device), data[1].to(device) #data[0], data[1]\n",
    "                    outputs = self.model(images)\n",
    "                    _, predictions = torch.max(outputs, 1)\n",
    "                    # collect the correct predictions for each class\n",
    "                    total += labels.shape[0]\n",
    "                    correct += (predictions == labels).sum().item()\n",
    "\n",
    "                    for label, prediction in zip(labels, predictions):\n",
    "                        if label == prediction:\n",
    "                            correct_pred[classes[label]] += 1\n",
    "                        total_pred[classes[label]] += 1\n",
    "\n",
    "            val_accuracy = 100 * correct / total\n",
    "            print(f'Epoch:', epoch + 1, f'Validation Epoch Accuracy:{val_accuracy}')\n",
    "                        \n",
    "            # print the summary for each class\n",
    "            print('Epoch:', epoch + 1, 'Correct predictions', correct_pred)\n",
    "            print('Epoch:', epoch + 1, 'Total predictions', total_pred)\n",
    "            print('Epoch:', epoch + 1, 'Correct predictions', correct_pred)\n",
    "            print('Epoch:', epoch + 1, 'Total predictions', total_pred)\n",
    "            \n",
    "            # inspect the time taken to train one epoch\n",
    "            d = time.time()-t\n",
    "            print('Fininsh Trainig Epoch', epoch, '!', 'Time used:', d)\n",
    "            \n",
    "            if save:\n",
    "                torch.save(self.model.state_dict(), os.path.join(self.save_path, \"model4.pt\"))\n",
    "                if val_accuracy > best_val_accuracy:\n",
    "                    torch.save(self.model.state_dict(), os.path.join(self.save_path, 'best.pt'))\n",
    "                    best_val_accuracy = val_accuracy\n",
    "\n",
    "        print('Done training!')                       \n",
    "\n",
    "    \n",
    "    def evaluate(self):\n",
    "        # for evaluating the test dataset if there were any.\n",
    "        try:\n",
    "            assert os.path.exists(os.path.join(self.save_path, 'best.pt'))\n",
    "            \n",
    "        except:\n",
    "            print('Please train first')\n",
    "            return\n",
    "        \n",
    "        self.model.load_state_dict(torch.load(os.path.join(self.save_path, 'best.pt')))\n",
    "        self.model.eval()\n",
    "        \n",
    "    def get_dataloaders(self, train_image_paths, train_labels, valid_image_paths, valid_labels, train_transforms=False, batch_size=32, shuffle=True):\n",
    "        train_dataset = Dataset(train_image_paths,train_labels, train_transforms)\n",
    "        val_dataset = Dataset(valid_image_paths,valid_labels, train_transforms)\n",
    "        train_loader = DataLoader(train_dataset, batch_size, shuffle)\n",
    "        valid_loader = DataLoader(val_dataset, batch_size, shuffle = True)\n",
    "        return train_loader, valid_loader\n",
    "    \n",
    "    def grad_cam_on_input(self, img):\n",
    "        \n",
    "        try:\n",
    "            assert os.path.exists(os.path.join(self.save_path, 'best.pt'))\n",
    "\n",
    "        except:\n",
    "            print('It appears you are testing the model without training. Please train first')\n",
    "            return\n",
    "\n",
    "        self.model.load_state_dict(torch.load(os.path.join(self.save_path, 'best.pt')))\n",
    "\n",
    "\n",
    "        self.model.eval()\n",
    "        img = img.to('mps' if self.use_cuda else 'cpu')\n",
    "\n",
    "\n",
    "        out = self.model(img)\n",
    "\n",
    "        _, pred = torch.max(out, 1)\n",
    "\n",
    "        predicted_class = self.class_names[int(pred)]\n",
    "        print(f'Predicted class was {predicted_class}')\n",
    "\n",
    "        out[:, pred].backward()\n",
    "        gradients = self.model.get_gradient_activations()\n",
    "\n",
    "        print('Gradients shape: ', f'{gradients.shape}')\n",
    "\n",
    "        mean_gradients = torch.mean(gradients, [0, 2, 3]).cpu()\n",
    "        activations = self.model.get_final_conv_layer(img).detach().cpu()\n",
    "\n",
    "        print('Activations shape: ', f'{activations.shape}')\n",
    "\n",
    "        for idx in range(activations.shape[1]):\n",
    "            activations[:, idx, :, :] *= mean_gradients[idx]\n",
    "\n",
    "        final_heatmap = np.maximum(torch.mean(activations, dim=1).squeeze(), 0)\n",
    "\n",
    "        final_heatmap /= torch.max(final_heatmap)\n",
    "\n",
    "        return final_heatmap\n",
    "\n",
    "    def trained_kernel_viz(self):\n",
    "        \n",
    "        all_layers = [0, 3]\n",
    "        all_filters = []\n",
    "        for layer in all_layers:\n",
    "\n",
    "            filters = self.model.conv_model[layer].weight\n",
    "            all_filters.append(filters.detach().cpu().clone()[:8, :8, :, :])\n",
    "\n",
    "        for filter_idx in range(len(all_filters)):\n",
    "\n",
    "            filter = all_filters[filter_idx]\n",
    "            print(filter.shape)\n",
    "            filter = filter.contiguous().view(-1, 1, filter.shape[2], filter.shape[3])\n",
    "            image = show_img(make_grid(filter))\n",
    "            image = 255 * image\n",
    "            cv2.imwrite(os.path.join(self.kernel_path, f'filter_layer{all_layers[filter_idx]}.jpg'), image)\n",
    "    \n",
    "\n",
    "    def activations_on_input(self, img):\n",
    "        \n",
    "        img = img.to('mps' if self.use_cuda else 'cpu')\n",
    "\n",
    "        all_layers = [0,3,6,8,10]\n",
    "        all_viz = []\n",
    "        \n",
    "        # looking at the outputs of the relu\n",
    "        for each in all_layers:\n",
    "\n",
    "            current_model = self.model.conv_model[:each+1]\n",
    "            current_out = current_model(img)\n",
    "            all_viz.append(current_out.detach().cpu().clone()[:, :64, :, :])\n",
    "\n",
    "        for viz_idx in range(len(all_viz)):\n",
    "\n",
    "            viz = all_viz[viz_idx]\n",
    "            viz = viz.view(-1, 1, viz.shape[2], viz.shape[3])\n",
    "            image = show_img(make_grid(viz))\n",
    "            image = 255 * image\n",
    "            cv2.imwrite(os.path.join(self.activations_path, f'sample_layer{all_layers[viz_idx]}.jpg'), image)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daae6962-2f51-4ae6-b31f-bcdbd1a72509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zihuiouyang/pytorch-test/env/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/zihuiouyang/pytorch-test/env/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B7_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B7_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "example_model = models.efficientnet_b7(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77d4a18f-b015-455b-a662-530abba5b22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferEffiNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base_effi_net = models.efficientnet_b7(pretrained=True)\n",
    "        self.conv_model = self.get_conv_layers()\n",
    "        self.avg_pool = self.transition_layer()\n",
    "        self.fc_model = self.get_fc_layers()\n",
    "        self.activate_training_layers()\n",
    "\n",
    "    def activate_training_layers(self):\n",
    "        for name, param in self.conv_model.named_parameters():\n",
    "            number = int(name.split('.')[1])\n",
    "            # for all layers except the last conv layer, set param.requires_grad = False\n",
    "            if number == 8:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        for name, param in self.fc_model.named_parameters():\n",
    "            # for all of these layers set param.requires_grad as True\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def get_conv_layers(self):\n",
    "        return self.base_effi_net.features\n",
    "\n",
    "    def transition_layer(self):\n",
    "        return self.base_effi_net.avgpool\n",
    "\n",
    "    def get_fc_layers(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(in_features=2560, out_features=1024, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(in_features=1024, out_features=512, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=512, out_features=2, bias=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_model(x)   #call the conv layers\n",
    "        x = self.avg_pool(x)  #call the avg pool layer\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_model(x)  #call fully connected layers  \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "643cbe5d-389c-4cf6-bfbb-9ef192749bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [01:25<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training Epoch Accuracy:87.14061683220073\n",
      "Epoch: 1 Validation Epoch Accuracy:92.06680584551148\n",
      "Epoch: 1 Correct predictions {0: 234, 1: 207}\n",
      "Epoch: 1 Total predictions {0: 248, 1: 231}\n",
      "Epoch: 1 Correct predictions {0: 234, 1: 207}\n",
      "Epoch: 1 Total predictions {0: 248, 1: 231}\n",
      "Fininsh Trainig Epoch 0 ! Time used: 95.06844997406006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [01:24<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Training Epoch Accuracy:96.86356508102457\n",
      "Epoch: 2 Validation Epoch Accuracy:95.82463465553236\n",
      "Epoch: 2 Correct predictions {0: 228, 1: 231}\n",
      "Epoch: 2 Total predictions {0: 248, 1: 231}\n",
      "Epoch: 2 Correct predictions {0: 228, 1: 231}\n",
      "Epoch: 2 Total predictions {0: 248, 1: 231}\n",
      "Fininsh Trainig Epoch 1 ! Time used: 94.14630484580994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [01:24<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Training Epoch Accuracy:97.90904338734971\n",
      "Epoch: 3 Validation Epoch Accuracy:98.74739039665971\n",
      "Epoch: 3 Correct predictions {0: 243, 1: 230}\n",
      "Epoch: 3 Total predictions {0: 248, 1: 231}\n",
      "Epoch: 3 Correct predictions {0: 243, 1: 230}\n",
      "Epoch: 3 Total predictions {0: 248, 1: 231}\n",
      "Fininsh Trainig Epoch 2 ! Time used: 93.78570294380188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [01:24<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Training Epoch Accuracy:97.8044955567172\n",
      "Epoch: 4 Validation Epoch Accuracy:99.16492693110648\n",
      "Epoch: 4 Correct predictions {0: 246, 1: 229}\n",
      "Epoch: 4 Total predictions {0: 248, 1: 231}\n",
      "Epoch: 4 Correct predictions {0: 246, 1: 229}\n",
      "Epoch: 4 Total predictions {0: 248, 1: 231}\n",
      "Fininsh Trainig Epoch 3 ! Time used: 93.99803185462952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [01:24<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Training Epoch Accuracy:98.74542603240982\n",
      "Epoch: 5 Validation Epoch Accuracy:99.58246346555323\n",
      "Epoch: 5 Correct predictions {0: 246, 1: 231}\n",
      "Epoch: 5 Total predictions {0: 248, 1: 231}\n",
      "Epoch: 5 Correct predictions {0: 246, 1: 231}\n",
      "Epoch: 5 Total predictions {0: 248, 1: 231}\n",
      "Fininsh Trainig Epoch 4 ! Time used: 93.98192000389099\n",
      "Done training!\n"
     ]
    }
   ],
   "source": [
    "name = 'TransferEffiNet'\n",
    "classes = [i for i in range(2)]\n",
    "transforms = get_transform('effinet')\n",
    "dataloaders = {'train_image_paths': train_image_paths, 'train_labels' : train_labels, 'valid_image_paths': valid_image_paths, 'valid_labels':valid_labels, 'transforms':transforms}\n",
    "parameters = {'lr': 0.001, 'epochs' : 5, 'batch_size':32, 'shuffle':False, 'class_names':classes}\n",
    "\n",
    "model = TransferEffiNet()\n",
    "classifier = Classifier(name, model, dataloaders, parameters, use_cuda=True)\n",
    "classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3a719ef-55c9-4efe-a37f-b403b8339c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_paths = []\n",
    "filename = '/Users/zihuiouyang/Downloads/images/testing/notflip'\n",
    "test_image_paths.append(glob.glob(filename + '/*'))\n",
    "filename1 = '/Users/zihuiouyang/Downloads/images/testing/flip'\n",
    "test_image_paths.append(glob.glob(filename1 + '/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "903a45cc-d168-4947-bf38-f162cafc5d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_paths1 = [item for sublist in test_image_paths for item in sublist]\n",
    "test_image_paths1 = natsort.natsorted(test_image_paths1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "089c900f-18fe-4fee-9d53-361fd1bc6992",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = []\n",
    "for i in range(307):\n",
    "    l1.append(0)\n",
    "for i in range(290):\n",
    "    l1.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eeb279ae-6ad9-4ca9-b1c7-672bac2315de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(model, train_transforms, batch_size, use_cuda=True):\n",
    "    test_dataset = Dataset(test_image_paths1, l1, train_transforms)\n",
    "    test_loader = DataLoader(test_dataset, batch_size, shuffle = False)\n",
    "    model = model.to('mps' if use_cuda else 'cpu')\n",
    "    pr = []\n",
    "    pred = []\n",
    "    l = []\n",
    "    # again no gradients needed\n",
    "    t = time.time()\n",
    "    negative_examples = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            l.append(labels)\n",
    "            outputs = model(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            m = F.softmax(outputs, dim=1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                pred.append(prediction)\n",
    "            for p in m:\n",
    "                pr.append(p)\n",
    "                \n",
    "    processtime = time.time()-t\n",
    "    print('processtime', processtime)\n",
    "    return l, pred, pr, processtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08a6727c-b741-4991-a318-e1e4391cbd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processtime 9.641700744628906\n"
     ]
    }
   ],
   "source": [
    "save_path = os.getcwd()\n",
    "best_effi = TransferEffiNet()\n",
    "best_effi.load_state_dict(torch.load(os.path.join(save_path, 'best.pt')))\n",
    "transforms = get_transform('effinet')\n",
    "batch_size = 32\n",
    "y_test_true, y_test_predicted, pr, time = get_pred(best_effi, transforms, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82c933d2-db63-4192-9bde-7dedb5292205",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m y_test_true \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m y_test_true \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_test_true)):\n\u001b[0;32m----> 7\u001b[0m     y_test_predicted[i] \u001b[38;5;241m=\u001b[39m \u001b[43my_test_predicted\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
     ]
    }
   ],
   "source": [
    "for i in range(len(y_test_true)):\n",
    "    y_test_true[i] = y_test_true[i].cpu()\n",
    "for i in range(len(y_test_true)):\n",
    "    y_test_true[i] = y_test_true[i].data.numpy()\n",
    "y_test_true = [item for sublist in y_test_true for item in sublist]\n",
    "for i in range(len(y_test_true)):\n",
    "    y_test_predicted[i] = y_test_predicted[i].cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79a315b9-f247-4415-b752-6044a2abb510",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_test_true)):\n",
    "    y_test_predicted[i] = y_test_predicted[i].cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbe00584-d069-4292-880e-5cd8d5f10996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a863c368-44f9-4144-9255-e92620917641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9965635738831615"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test_true, y_test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "edd33423-26eb-440a-8e98-277f9aeb9739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[305,   2],\n",
       "       [  0, 290]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test_true, y_test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57acf207-1678-4484-88c4-923eb101df3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in range(len(y_test_true)):\n",
    "    a.append(np.array([y_test_predicted[0]])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3b90e7-af9a-4aaa-9143-4a114079bb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb865e1-a98a-479b-abdb-9d7fa506f1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
